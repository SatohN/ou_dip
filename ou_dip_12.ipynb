{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ou_dip_12.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOsntRevrLonUl0hMVaGgwM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## お手軽 Convolutional neural network (CNN)\n","\n","### 概要\n","**簡易的な実装**なので、精度・速度ともに実用には不向きです。学習には、GPUを使っても多少時間がかるので、気長に実行してください（Colabのタイムアウトに注意してください）。\n","\n","同シリーズ「お手軽BoVW」と同じデータセット（Caltech101）を使っていますので、比較しながら実行すると良いでしょう。また、コードを追うと、近年のモダンな深層学習ライブラリ（ここではPyTorchを使います）の便利さも実感できるかと思います。\n","\n","ちなみに、実はCaltech101（約100クラス・約10000画像）は**小さいデータセット**なので、あまり深層学習には向かないです。が、これくらいのデータセットしか集められないこと、よくありますよね。。。そういう場合にどうするのか、よくある方法も紹介します。\n","\n","参考：https://debuggercafe.com/getting-95-accuracy-on-the-caltech101-dataset-using-deep-learning/\n","\n","### 準備\n","**GPUインスタンスで実行してください：**\n","1.   「ランタイム」から「ランタイムのタイプを変更」\n","2.   ハードウェアアクセラレータを「GPU」に\n","\n","最後のセルなどで使っているサンプル画像は、githubリポジトリ`ou_dip/bovw/`以下にありますので、ランタイム起動後にアップロードしてください。\n","\n","\n"],"metadata":{"id":"BunU0X0zWamm"}},{"cell_type":"code","source":["## 大量データにアクセスする場合、Googleドライブ上だと遅くなるのでコメントアウトしています\n","# Googleドライブへのマウント（Colab用コード）\n","# from google.colab import drive\n","# drive.mount('/content/drive')\n","# %cd \"/content/drive/My Drive/Colab Notebooks/ou_dip/\"\n","\n","import cv2\n","import numpy as np  \n","from PIL import Image\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import time\n","import pickle\n","import os\n","\n","def imshow(img):\n","  if img.ndim == 3:\n","    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n","    display(Image.fromarray(img))\n","  else:\n","    display(Image.fromarray(img))"],"metadata":{"id":"G-lKE9DMW4NF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step 0: 入力データの準備\n","ここでは、Caltech101というデータセットを使う。101クラスのデータセットで、およそ10000枚の画像が含まれる。\n","\n","http://www.vision.caltech.edu/Image_Datasets/Caltech101/\n","\n","ここでは、8割をtrain、1割をvalidation、1割をtestにランダムに分割する。ここで、各データセットは以下のものを指す（資料によっては呼び方が違ったりする）\n","\n","* training datasetは、機械学習器の学習に使う\n","* validation datasetは、学習器のハイパーパラメータ（繰り返し回数など）選択などに使う。\n","* test datasetは、学習器の精度評価などに使う。**注意：ハイパーパラメータ探索にtestデータセットを使ってはいけない！**\n","\n","データのダウンロードには、PyTorch（とtorchvision）という深層学習向けライブラリを使っている。\n"],"metadata":{"id":"sRUGyMKqW7nE"}},{"cell_type":"markdown","source":["### 共通の設定"],"metadata":{"id":"g_xkvF0LbGb-"}},{"cell_type":"code","source":["from tqdm.notebook import tqdm\n","import random\n","\n","# バッチサイズ（1つのミニバッチに含まれるサンプル数）\n","batch_size = 64\n","\n","# 学習するエポック数\n","n_epochs = 10"],"metadata":{"id":"tch9sK6_bLV3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision\n","import torchvision.transforms as transforms\n","\n","if torch.cuda.is_available():\n","    device = 'cuda' # GPUが使えるなら使う\n","else:\n","    device = 'cpu'\n","\n","# 画像読み込み時の変換の指定。これを入れると、画像読み込み時に自動化してくれる\n","transforms = transforms.Compose([\n","    #transforms.ToPILImage(),        # PILImageという画像の保持形式にし、\n","    torchvision.transforms.Grayscale(num_output_channels=3),  # グレイスケール画像のチャンネル数を強制的に3にする\n","    transforms.Resize((224, 224)),  # 224x224にリサイズし、\n","    transforms.ToTensor(),          # PyTorchの多次元配列の型（Tensor型）にし、\n","    transforms.Normalize(mean = [0.485,0.456,0.406], std=[0.229,0.224,0.225]),  # 各チャネルを、引数で与えたmeanとstdで正規化する\n","])\n","\n","# Caltech101データセットのダウンロード\n","dataset = torchvision.datasets.Caltech101(root='./data', download=True, transform=transforms)\n","categories = dataset.categories # category list\n","\n","# train, val, testデータセットの作成（ランダム分割）\n","n_samples = len(dataset)\n","train_size = int(n_samples * 0.8)\n","val_size = int((n_samples-train_size)*0.5)\n","test_size = n_samples - train_size - val_size\n","print(\"train_size:\", train_size, \"val_size:\", val_size, \"test_size\", test_size)\n","\n","# split dataset into training and test datasets with fixed random seed (42)\n","trainval_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size+val_size, test_size], generator=torch.Generator().manual_seed(42))\n","train_dataset, val_dataset = torch.utils.data.random_split(trainval_dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n","\n","# PyTorchにはDataLoaderというクラスがあり、iterationごとにデータを読み込む部分を隠蔽できて便利\n","## batch_sizeはミニバッチのサイズの指定。num_workersは読み込みの並列化に使う。\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n","val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=2)"],"metadata":{"id":"tSFidqHsW7b3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device  # ここがcudaになっていることを確認しましょう（なっていなかったらGPUインスタンスに切り替えてください）"],"metadata":{"id":"Sz3gHxyycDd0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step 1: モデルの構築\n","ResNet34（34層のResNetモデル）を使ってみます。ResNetは、層の数によって17,50,101,152もあり、torchvisionのmodelsで定義されています。他にもいろいろなモデルがありますので、興味があれば切り替えて使ってみてください。\n","\n","https://pytorch.org/vision/stable/models.html"],"metadata":{"id":"X_Q-5-7zcalf"}},{"cell_type":"code","source":["# torchvisionで提供されているResNet34の中身を見てみる\n","print(torchvision.models.resnet34())"],"metadata":{"id":"TVAjBrlAgRrn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ResNet34をベースにしたネットワーク構造\n","class ResNet34_scratch(nn.Module):\n","    def __init__(self):\n","        super(ResNet34_scratch, self).__init__()\n","        self.model  = torchvision.models.resnet34()\n","\n","        # avgpoolの出力が512次元。元のResNetは1000クラス分類用のfc層が最後についているので、\n","        # 最終層（全結合層: fc）だけ、分類したいクラス数（101）に合わせて変更するため上書きしてしまう\n","        # 元のFC層の定義　(fc): Linear(in_features=512, out_features=1000, bias=True)\n","        self.model.fc = nn.Linear(512, len(dataset.categories), bias=True)\n","    \n","    def forward(self, x):\n","        # forward passの定義。\n","        # backwardの計算（勾配を求めて云々。。。）はPyTorchが自動でやってくれる。\n","        \n","        # xに、入力データが入ってくる。あとは、__init__で定義したモジュールを組み合わせて完成\n","        out = self.model(x) \n","        return out\n","\n","model = ResNet34_scratch().to(device)\n"],"metadata":{"id":"nqiOJ6lBaIrh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torchsummary import summary\n","print(summary(model, input_size=(3, 224, 224)))  # 引数で指定された入力が与えられた場合のパラメータ数などを計算してくれる"],"metadata":{"id":"KgCX0QW-csfv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step 2: 学習\n"],"metadata":{"id":"6IIIMGiMlEYX"}},{"cell_type":"code","source":["# 損失関数。 nn.CrossEntropyLoss() はSoftmaxとCross entropyを同時に計算してくれる\n","loss_criterion = nn.CrossEntropyLoss() \n","\n","# optimizerの指定。Adamを使うことにする。引数で指定しているのは最適化対象のパラメータと学習率。\n","## 最適化対象については、model.parameters()で、モデル内の全パラメータを渡せる\n","optimizer = optim.Adam(model.parameters(), lr=1e-4)\n","\n","# 任意のデータセット全体に対し、（パラメータの最適化をせず）lossだけ計算する関数\n","def validate(model, dataloader):\n","  model.eval()  # evalモード\n","  losses = 0.0\n","  correct_preds = 0\n","  with torch.no_grad(): # 最適化しないので、勾配を計算しない\n","    for data in dataloader:\n","      input, label = data[0].to(device), data[1].to(device)\n","      outputs = model(input)\n","      loss = loss_criterion(outputs, label)\n","      \n","      losses += loss.item()\n","      _, preds = torch.max(outputs.data, 1)\n","      correct_preds += (preds == label).sum().item()\n","    \n","    avg_loss = losses/len(dataloader)\n","    accuracy = float(correct_preds)/len(dataloader.dataset)\n","    \n","    return avg_loss, accuracy"],"metadata":{"id":"BTf7cstlxNFs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 学習\n","loss_train , acc_train = [], []\n","loss_val , acc_val = [], []\n","\n","print('Training')\n","start = time.perf_counter()\n","\n","for epoch in range(n_epochs):\n","    model.train()\n","    losses = 0.0\n","    correct_preds = 0\n","    \n","    print(\"Epoch\", epoch, \"/\", n_epochs-1)\n","    for data in tqdm(train_loader):\n","        input, label = data[0].to(device), data[1].to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(input)\n","\n","        loss = loss_criterion(outputs, label)\n","        losses += loss.item()\n","        _, preds = torch.max(outputs.data, 1)\n","        correct_preds += (preds == label).sum().item()\n","\n","        loss.backward()\n","        optimizer.step()\n","        \n","    avg_loss = losses/len(train_loader)\n","    acc = float(correct_preds)/len(train_loader.dataset)    \n","    print(f\"Train Loss: {avg_loss:.4f}, Train Acc: {acc:.4f}\")\n","\n","    print('Validating...')\n","    vl, va = validate(model, val_loader)\n","    print(f'Val Loss: {vl:.4f}, Val Acc: {va:.4f}')\n","    \n","    loss_train.append(avg_loss)\n","    acc_train.append(acc)\n","    loss_val.append(vl)\n","    acc_val.append(va)\n","\n","    # 各エポック終了時のモデルを保存しておく\n","    path = str(epoch) + \".pt\"\n","    torch.save({'model_state_dict': model.state_dict(),'optimizer_state_dict': optimizer.state_dict()}, path)\n","\n","print(\"Elapsed time:\",time.perf_counter()-start,\"[s]\")"],"metadata":{"id":"KWR12U-SjZfI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["best_epoch = loss_val.index(min(loss_val))  # eval_setに対するlossが最も小さかったエポック数\n","print(\"Best epoch:\", best_epoch, \"train loss:\", loss_train[best_epoch], \"val loss:\", loss_val[best_epoch])\n","\n","# 学習曲線の描画\n","fig = plt.figure()\n","ax1 = fig.add_subplot()\n","ax1.set_xlabel('iteration')\n","ax1.set_ylabel('loss')\n","\n","ax1.plot(loss_train, label='train loss')\n","ax1.plot(loss_val, label='val loss')\n","plt.legend()\n","plt.show()\n","plt.close()"],"metadata":{"id":"sYBK_Al-okNx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step 3: 学習済みモデルを使った推論\n","\n","学習済みのモデルを使って、\n","\n","*   testデータセットに対する推論をして精度評価をする\n","*   任意の画像を入力として推論する\n","\n","**validation datasetは既にハイパーパラメータの設定（best iterationの選択）に使ったので評価に使ってはいけない**\n","\n"],"metadata":{"id":"flQPNHntyi-D"}},{"cell_type":"markdown","source":["### testデータセットを使った評価\n","\n","最もvalidation setに対する精度が良かったエポックのモデルを使う\n","\n","#### Tips: 学習時と評価時の振る舞いの変化\n","\n","trainに対する精度が、学習中のログとして出力したものと異なる（多くの場合、学習時より精度が高くなる）ことがある。学習中と評価時は一部の処理の内容が異なるためである。例えば、\n","* batch normalizationは、学習時はミニバッチ内の平均・分散を使うが、評価時は全データを使う\n","* dropoutは、学習時のみ導入される\n","\n","これらの振る舞いは、model.train()とmodel.eval()で切り替えることができる\n","\n","ちなみに、学習曲線をみると、たまに学習に使っていないvalidation setに対する精度がtrainに対するものより良いことがある。この理由も上記によることが多い。"],"metadata":{"id":"lqAYUT0HymFd"}},{"cell_type":"code","source":["checkpoint = torch.load(str(best_epoch) + \".pt\")\n","model.load_state_dict(checkpoint['model_state_dict'])\n","\n","test_loss, test_acc = validate(model, test_loader)\n","train_loss, train_acc = validate(model, train_loader)\n","val_loss, val_acc = validate(model, val_loader)\n","\n","print(\"---Accuracy evaluation---\")\n","\n","print(\"Test accuracy:\",test_acc)\n","print(\"Train accuracy (for reference)':\",train_acc)\n","print(\"Val accuracy (for reference):\",val_acc)"],"metadata":{"id":"13Jr7m-5yiyM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 任意の画像に対する予測"],"metadata":{"id":"3Vlyn9F41UR1"}},{"cell_type":"code","source":["path = \"wrightflyer.jpg\"\n","\n","image = Image.open(path)\n","image = transforms(image).to(device)\n","image = image.unsqueeze(0)\n","\n","with torch.no_grad():\n","    output = model(image)\n","    preds = torch.max(output.data, 1).indices.cpu().numpy()\n","    pred = preds[0] # 1つの画像しか入力していないので、最初の要素をとってくればOK\n","\n","print(\"Predicted as\", categories[pred], \", category id:\",pred)\n","imshow(cv2.imread(path))"],"metadata":{"id":"P0n3UqtN1Tkm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step 4: 大規模データセットで学習されたモデルをfine-tuneする\n","\n","Caltech101は**小さな**データセットであるため、これだけを学習に使うとすぐにtrainデータセットへの過適合を起こしてしまう。\n","\n","よくある物体（飛行機とか人とか・・・）の認識に使うような画像を使う場合、ImageNetという大規模な画像データセット（と1000クラスの分類用のラベル）を使って学習されたパラメータを初期値として、あるいはそのまま利用して学習（fine-tuning）することが多い。\n","\n","Fine-tuningといっても様々な方法があるが、ここでは最後の全結合層 (fc) のパラメータのみを最適化対象として、その他は事前学習された（pretrained）重みをそのまま使うことにする。\n","\n","今回のfine-tuning方法がうまくいくのは、ImageNetとCaltech101のドメイン（タスクや含まれる画像）がかなり似通っているから。事前学習に使われたドメインとfine-tuneにつかうデータセットのドメインもう少し離れている場合は、もうすこし多くのパラメータを更新対象とするようなことを検討すると良い。（あまりにかけ離れたドメインの場合は、pretrained modelを使うよりも、ランダム重みから学習する方が良いこともある）\n"],"metadata":{"id":"ksKVzHh-28yp"}},{"cell_type":"code","source":["# ResNet34をベースにしたネットワーク構造\n","class ResNet34_finetune(nn.Module):\n","    def __init__(self):\n","        super(ResNet34_finetune, self).__init__()\n","        self.model = torchvision.models.resnet34(pretrained=True) # pretrained=Trueとすると、ImageNetで事前学習された重みを読み込む\n","\n","        # 事前学習されたパラメータのうち、前の方の層は固定したい。\n","        for param in model.parameters():  # modelに含まれる全パラメータについて\n","          param.requires_grad = False # requires_grad = Falseとすると、そのパラメータを固定してくれる。\n","        \n","        # avgpoolの出力が512次元。元のResNetは1000クラス分類用のfc層が最後についているので、\n","        # 最終層（全結合層: fc）だけ、分類したいクラス数（101）に合わせて変更するため上書きしてしまう\n","        # 元のFC層の定義　(fc): Linear(in_features=512, out_features=1000, bias=True)\n","        self.model.fc = nn.Linear(512, len(dataset.categories), bias=True)       \n","        self.model.fc.weight.requires_grad = True # ここは更新対象にしたい\n","        self.model.fc.bias.requires_grad = True # ここは更新対象にしたい\n","    \n","    def forward(self, x):\n","        # forward passの定義。\n","        # backwardの計算（勾配を求めて云々。。。）はPyTorchが自動でやってくれる。\n","        \n","        # xに、入力データが入ってくる。あとは、__init__で定義したモジュールを組み合わせて完成\n","        out = self.model(x) \n","        return out\n","\n","model_ft = ResNet34_finetune().to(device)\n","\n","optimizer = optim.Adam(model_ft.parameters(), lr=1e-4)\n","#optimizer = optim.Adam(filter(lambda p: p.requires_grad, model_ft.parameters()), lr=1e-4)\n","\n","# 学習\n","loss_train , acc_train = [], []\n","loss_val , acc_val = [], []\n","\n","print('Training')\n","start = time.perf_counter()\n","\n","for epoch in range(n_epochs):\n","    model_ft.train()\n","    losses = 0.0\n","    correct_preds = 0\n","    \n","    print(\"Epoch\", epoch, \"/\", n_epochs-1)\n","    for data in tqdm(train_loader):\n","        input, label = data[0].to(device), data[1].to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model_ft(input)\n","\n","        loss = loss_criterion(outputs, label)\n","        losses += loss.item()\n","        _, preds = torch.max(outputs.data, 1)\n","        correct_preds += (preds == label).sum().item()\n","\n","        loss.backward()\n","        optimizer.step()\n","        \n","    avg_loss = losses/len(train_loader)\n","    acc = float(correct_preds)/len(train_loader.dataset)    \n","    print(f\"Train Loss: {avg_loss:.4f}, Train Acc: {acc:.4f}\")\n","\n","    print('Validating...')\n","    vl, va = validate(model_ft, val_loader)\n","    print(f'Val Loss: {vl:.4f}, Val Acc: {va:.4f}')\n","    \n","    loss_train.append(avg_loss)\n","    acc_train.append(acc)\n","    loss_val.append(vl)\n","    acc_val.append(va)\n","\n","    # 各エポック終了時のモデルを保存しておく\n","    path = str(epoch) + \"_finetune.pt\"\n","    torch.save({'model_state_dict': model_ft.state_dict(),'optimizer_state_dict': optimizer.state_dict()}, path)\n","\n","print(\"Elapsed time:\",time.perf_counter()-start,\"[s]\")"],"metadata":{"id":"G-IGIf0v6PNX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["best_epoch = loss_val.index(min(loss_val))  # eval_setに対するlossが最も小さかったエポック数\n","print(\"Best epoch:\", best_epoch, \"train loss:\", loss_train[best_epoch], \"val loss:\", loss_val[best_epoch])\n","\n","# 学習曲線の描画\n","fig = plt.figure()\n","ax1 = fig.add_subplot()\n","ax1.set_xlabel('iteration')\n","ax1.set_ylabel('loss')\n","\n","ax1.plot(loss_train, label='train loss')\n","ax1.plot(loss_val, label='val loss')\n","plt.legend()\n","plt.show()\n","plt.close()"],"metadata":{"id":"OdQyXRJRAkHe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 最もvalidation setに対する精度が良かったエポックのモデルを使って評価\n","checkpoint = torch.load(str(best_epoch) + \"_finetune.pt\")\n","model_ft.load_state_dict(checkpoint['model_state_dict'])\n","\n","test_loss, test_acc = validate(model_ft, test_loader)\n","train_loss, train_acc = validate(model_ft, train_loader)\n","val_loss, val_acc = validate(model_ft, val_loader)\n","\n","print(\"---Accuracy evaluation for fine-tuned model---\")\n","\n","print(\"Test accuracy:\",test_acc)\n","print(\"Train accuracy (for reference)':\",train_acc)\n","print(\"Val accuracy (for reference):\",val_acc)"],"metadata":{"id":"OgtIJeBa8wew"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["path = \"wrightflyer.jpg\"\n","\n","image = Image.open(path)\n","image = transforms(image).to(device)\n","image = image.unsqueeze(0)\n","\n","with torch.no_grad():\n","    output = model_ft(image)\n","    preds = torch.max(output.data, 1).indices.cpu().numpy()\n","    pred = preds[0] # 1つの画像しか入力していないので、最初の要素をとってくればOK\n","\n","print(\"Predicted as\", categories[pred], \", category id:\",pred)\n","imshow(cv2.imread(path))"],"metadata":{"id":"9zWsso0KAOHL"},"execution_count":null,"outputs":[]}]}