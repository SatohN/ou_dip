{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ou_dip_11.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOyNGBfttYBU7ELCdvkQnd2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## お手軽 Bag of Visual Words (BoVW)\n","\n","### 概要\n","Bag of Features (BoF) とも呼ばれます。\n","**簡易的な実装**なので、精度・速度ともに実用には不向きです。多くの部分で並列処理などを実装していないので遅いです。気長に実行してください。（Colabのタイムアウトに注意してください）。\n","\n","BoVW参考：\n","\n","* https://hazm.at/mox/machine-learning/computer-vision/recipes/similar-image-retrieval.html\n","* https://n-hidekey.hatenadiary.org/entry/20111120/1321803326\n","\n","### 準備\n","**GBDTの学習にGPUを使う場合はGPUインスタンスで実行してください：**\n","1.   「ランタイム」から「ランタイムのタイプを変更」\n","2.   ハードウェアアクセラレータを「GPU」に\n","\n","途中でキャッシュされるpickleファイル（'*.pkl'）をダウンロードしておき、次回のランタイム起動後にアップロードして使うと、時間のかかる処理をスキップできます。\n","\n","pklファイルおよびサンプル画像などは、githubリポジトリ`ou_dip/bovw/`以下にありますので、ランタイム起動後にアップロードしてください。\n","\n","\n"],"metadata":{"id":"NOR-aaliZc-n"}},{"cell_type":"code","source":["## 大量データにアクセスする場合、Googleドライブ上だと遅くなるのでコメントアウトしています\n","# Googleドライブへのマウント（Colab用コード）\n","# from google.colab import drive\n","# drive.mount('/content/drive')\n","# %cd \"/content/drive/My Drive/Colab Notebooks/ou_dip/\"\n","\n","# 伝統的な方法に則って、SIFT特徴量を使ってみる（2019年に特許が切れたため、最新版にはSIFTが入っている）\n","!pip install opencv-contrib-python==4.5.4.60\n","\n","# GBDTのライブラリ（CatBoost）のインストール\n","!pip install catboost\n","\n","import cv2\n","import numpy as np  \n","from PIL import Image\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import time\n","import pickle\n","import os\n","\n","def imshow(img):\n","  if img.ndim == 3:\n","    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n","    display(Image.fromarray(img))\n","  else:\n","    display(Image.fromarray(img))"],"metadata":{"id":"nf3R-mZTDknb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step 0: 入力データの準備\n","ここでは、Caltech101というデータセットを使う。101クラスのデータセットで、およそ10000枚の画像が含まれる。\n","\n","http://www.vision.caltech.edu/Image_Datasets/Caltech101/\n","\n","ここでは、8割をtrain、1割をvalidation、1割をtestにランダムに分割する。ここで、各データセットは以下のものを指す（資料によっては呼び方が違ったりする）\n","\n","* training datasetは、機械学習器の学習に使う\n","* validation datasetは、学習器のハイパーパラメータ（繰り返し回数など）選択などに使う。\n","* test datasetは、学習器の精度評価などに使う。**注意：ハイパーパラメータ探索にtestデータセットを使ってはいけない！**\n","\n","データのダウンロードには、PyTorch（とtorchvision）という深層学習向けライブラリを使っている。深層学習用途じゃなくても、代表的なデータセットの自動ダウンロードなどができるので楽。"],"metadata":{"id":"YSpqJinqXKMB"}},{"cell_type":"code","source":["import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","\n","# Caltech101データセットのダウンロード\n","dataset = torchvision.datasets.Caltech101(root='./data', download=True, transform=None)\n","categories = dataset.categories # category list\n","\n","# train, val, testデータセットの作成（ランダム分割）\n","if os.path.exists('datasets.pkl'):  # 各データセットに含まれるファイルのIDが保存されている\n","  print(\"Loading datasets from 'datasts.pkl'.\")\n","  with open('datasets.pkl','rb') as f:  \n","    train_dataset, val_dataset, test_dataset = pickle.load(f)\n","\n","else:\n","  n_samples = len(dataset)\n","  train_size = int(n_samples * 0.8)\n","  val_size = int((n_samples-train_size)*0.5)\n","  test_size = n_samples - train_size - val_size\n","  print(\"train_size:\", train_size, \"val_size:\", val_size, \"test_size\", test_size)\n","\n","  # split dataset into training and test datasets with fixed random seed (42)\n","  trainval_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size+val_size, test_size], generator=torch.Generator().manual_seed(42))\n","  train_dataset, val_dataset = torch.utils.data.random_split(trainval_dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n","\n","  with open('datasets.pkl', 'wb') as f:\n","      pickle.dump([train_dataset, val_dataset, test_dataset], f)"],"metadata":{"id":"QVGUgK1iDpru"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 特定の画像の取り出し関数 (numpy array, grayscale)。データセットと画像IDを渡す\n","def single_image_loader(dataset,img_id):\n","  img = np.array(dataset[img_id][0])\n","  if img.ndim == 3:\n","    img = cv2.cvtColor(img,cv2.COLOR_RGB2GRAY)\n","  label_id = dataset[img_id][1]\n","\n","  return img, label_id\n","\n","# グリッド点上にキーポイントを作る関数\n","def create_dense_keypoints(image, detector, step, scale):\n","    if len(image.shape) == 3:\n","      gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","    else:\n","      gray_image = image\n","\n","    # Create dense keypoints\n","    keypoints = []\n","    rows, cols = gray_image.shape\n","    for y in range(0, rows, step):\n","      for x in range(0, cols, step):\n","        keypoints.append(cv2.KeyPoint(float(x), float(y), scale))\n","    \n","    return keypoints\n","\n","# ランダムな点上にキーポイントを作る関数\n","import random\n","def create_random_keypoints(image, detector, n_points, scale):\n","    if len(image.shape) == 3:\n","      gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","    else:\n","      gray_image = image\n","\n","    # Create random keypoints\n","    keypoints = []\n","    rows, cols = gray_image.shape\n","    for i in range(n_points):  \n","      y = random.randrange(rows)\n","      x = random.randrange(cols)\n","      keypoints.append(cv2.KeyPoint(float(x), float(y), scale))\n","    \n","    return keypoints"],"metadata":{"id":"5eKYWpH-FDnY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 共通の設定"],"metadata":{"id":"LVJ4kPVqWniW"}},{"cell_type":"code","source":["from tqdm.notebook import tqdm\n","import random\n","\n","# visual wordsのクラスタ数\n","n_clusters = 1000\n","\n","# feat_step画素ごとのグリッド点上で特徴量を計算\n","feat_step = 9  \n","\n","# 使用する特徴量\n","# feat = cv2.ORB_create()  # ORB特徴量\n","feat = cv2.SIFT_create()  # SIFT特徴量 (OpenCV4.4.0以降)"],"metadata":{"id":"MwC_E5LEWmbr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 参考：特徴点の可視化\n","img_id = 0\n","img, label_id = single_image_loader(train_dataset,img_id)\n","print(categories[label_id], \", category id:\",label_id)\n","imshow(img)\n","\n","print(\"Dense keypoints\")\n","kp = create_dense_keypoints(img,feat,step=feat_step,scale=feat_step)\n","img2 = cv2.drawKeypoints(img,kp,None,flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n","imshow(img2)\n","\n","print(\"Random keypoints\")\n","kp = create_random_keypoints(img,feat,n_points=100,scale=feat_step)\n","kp, desc = feat.compute(img, kp)\n","img2 = cv2.drawKeypoints(img,kp,None,flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n","imshow(img2)"],"metadata":{"id":"raED1xngomMp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step 1: Visual Wordsの作成\n","\n","SIFT特徴量（などの特徴量）のクラスタリングにより、Visual Words（辞書）を作成する。一つのクラスタが一つの「単語」となり、画像（文書）中での「単語」の出現頻度を使って分類などを行う。\n"],"metadata":{"id":"N0Z0JnzCT7kY"}},{"cell_type":"code","source":["if os.path.exists('vw.pkl'):\n","  with open('vw.pkl','rb') as f:  \n","    clusters = pickle.load(f)\n","  print(\"Loading\", clusters.shape[0], \"visual words from 'vw.pkl'.\")\n","\n","else:\n","  # 時間がかかるので、ランダムにサンプルする。\n","  # 学習用画像が6000枚くらいあるので、各20点くらいサンプリングすることにする。\n","  bowTrainer = cv2.BOWKMeansTrainer(n_clusters)\n","\n","  print(\"Extracting features from\",len(train_dataset),\"images...\")\n","  start = time.perf_counter()\n","  for img_id in tqdm(range(len(train_dataset))):  \n","    img, label_id = single_image_loader(train_dataset,img_id)\n","\n","    keypoints = create_random_keypoints(img,feat,n_points=20,scale=feat_step)\n","    keypoints, descriptors = feat.compute(img, keypoints)\n","\n","    bowTrainer.add(descriptors.astype(np.float32))\n","\n","  print(\"Elapsed time:\",time.perf_counter()-start,\"[s]\")\n","\n","  # 時間がかかるのでしばし待つ。\n","  print(\"Clustering\",bowTrainer.descriptorsCount(),\"features.\")\n","  start = time.perf_counter()\n","  clusters = bowTrainer.cluster() # k-means法によるクラスタリング\n","  print(\"Elapsed time:\",time.perf_counter()-start,\"[s]\")\n","\n","  with open('vw.pkl', 'wb') as f:\n","    pickle.dump(clusters, f)"],"metadata":{"id":"4Jur7BXqUEuP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(clusters.shape) # クラスタ数n_clustersの重心：SIFT特徴の場合128次元の特徴空間上に定義される\n","print(clusters)"],"metadata":{"id":"kPLl9gsigUOw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step 2: 学習データのBoVWヒストグラム特徴の作成\n","\n","学習データ（とvalidationデータ）の各画像についてVisual wordsのヒストグラム（各画像で、どのクラスタに当てはまる点が何点あるかのヒストグラム）を作成する。\n","\n","1. 画像中のグリッド上の点で特徴量（SIFTなど）を計算\n","2. Visual wordsのどの単語に当てはまるか計算（最近傍法）\n","3. Visual words出現確率のヒストグラムを生成→これをBoVW特徴とする\n","\n","\n"],"metadata":{"id":"H1GiKI75QLcz"}},{"cell_type":"code","source":["# BoVWの特徴量計算クラス（OpenCV）の設定\n","FLANN_INDEX_KDTREE = 1\n","index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n","search_params = {}\n","matcher = cv2.FlannBasedMatcher(index_params, search_params)\n","\n","# bag of visual words extractor\n","bowExtractor = cv2.BOWImgDescriptorExtractor(feat, matcher)\n","bowExtractor.setVocabulary(clusters)\n","\n","# データセット全画像のBoVWヒストグラム（と対応する出力）を計算する関数\n","def calc_probs(dataset):\n","  probs = []\n","  labels = []\n","  for img_id in tqdm(range(len(dataset))):  \n","    img, label_id = single_image_loader(dataset,img_id)\n","    \n","    # グリッド点上に特徴点を定義\n","    keypoints = create_dense_keypoints(img,feat,step=feat_step,scale=feat_step)\n","\n","    # SIFT特徴量を計算し、一番近いクラスタを見つけ（FLANNによる近似最近傍探索）、画像中の全点でヒストグラムを作る\n","    histogram = bowExtractor.compute(img, keypoints)[0]\n","\n","    probs.append(histogram) \n","    labels.append(label_id) \n","  return probs, labels\n","\n","# train, val各データセットに対してBoVW特徴を計算\n","if os.path.exists('bovw_train_val.pkl'):\n","  print(\"Loading  from 'bovw_train_val.pkl'.\")\n","  with open('bovw_train_val.pkl','rb') as f:  \n","    probs_train, labels_train, probs_val, labels_val = pickle.load(f)\n","\n","else:\n","  print(\"Extracting BoVW histograms from training dataset\")\n","  start = time.perf_counter()\n","  probs_train, labels_train = calc_probs(train_dataset) # TRAINING dataset\n","  print(\"Elapsed time:\",time.perf_counter()-start,\"[s]\")\n","\n","  print(\"Extracting BoVW histograms from validation dataset\")\n","  start = time.perf_counter()\n","  probs_val, labels_val = calc_probs(val_dataset) # VALIDATION dataset\n","  print(\"Elapsed time:\",time.perf_counter()-start,\"[s]\")\n","  \n","  with open('bovw_train_val.pkl', 'wb') as f:\n","    pickle.dump([probs_train, labels_train, probs_val, labels_val], f)"],"metadata":{"id":"QtKXfdFeKJEn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ヒストグラム（0番目の画像）の可視化\n","left = np.arange(0,1000)\n","plt.bar(left,probs_train[0])"],"metadata":{"id":"-T48kJARP8TA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step 3: GBDTの学習\n","\n","メジャーなGBDTのライブラリとして[XGBoost](https://xgboost.readthedocs.io/en/stable/), [LightGBM](https://lightgbm.readthedocs.io/en/latest/), [CatBoost](https://catboost.ai/en/docs/)などがあります。\n","\n","ここでは、（Colab上でGPUを使うのが最も簡単、という理由で）CatBoostを使うことにします。\n","\n","損失関数にはデフォルト設定のlog lossを使っています。いわゆる cross entropy lossと同じものです。\n","\n","参考：\n","\n","* https://catboost.ai/en/docs/concepts/python-usages-examples\n","* https://colab.research.google.com/github/catboost/tutorials/blob/master/tools/google_colaboratory_cpu_vs_gpu_tutorial.ipynb"],"metadata":{"id":"HVs2k4y7opYS"}},{"cell_type":"code","source":["from catboost import CatBoostClassifier\n","\n","x_train = np.vstack(probs_train)\n","x_val = np.vstack(probs_val)\n","\n","y_train = labels_train\n","y_val = labels_val\n"," \n","print(\"Training GBDT\")\n","start = time.perf_counter()\n","    \n","gbm = CatBoostClassifier(\n","  iterations=1000,    # イテレーション数\n","  learning_rate=0.1,  # 学習率\n","  task_type='GPU'     # CPUインスタンスを使う場合はここをコメントアウト\n",")\n","\n","gbm.fit(\n","    x_train, y_train,           # 学習データ（入力と出力の組）\n","    eval_set=(x_val, y_val),    # 学習時の\"評価\"につかうデータ（validation）→ early stoppingなどの指標になる\n","    #early_stopping_rounds=10,  # 10ラウンド経過してもeval_setに対する精度が上がらなければ打ち切り\n","    #use_best_model=True,       # 最も精度が高かったモデルを返す\n","    verbose=10\n",")\n","\n","print(\"Elapsed time:\",time.perf_counter()-start,\"[s]\")\n","  \n","gbm_results = gbm.get_evals_result()\n","best_iteration = gbm.get_best_iteration()  # eval_setに対するlossが最も小さかったイテレーション数\n","\n","# 学習曲線の描画\n","loss_train = gbm_results['learn']['MultiClass']       # 訓練誤差\n","loss_val = gbm_results['validation']['MultiClass']    # 汎化誤差\n","\n","fig = plt.figure()\n","ax1 = fig.add_subplot()\n","ax1.set_xlabel('iteration')\n","ax1.set_ylabel('log loss')\n","\n","ax1.plot(loss_train, label='train loss')\n","ax1.plot(loss_val, label='val loss')\n","plt.legend()\n","plt.show()\n","plt.close()\n","\n","print(\"Best iteration:\", best_iteration, \"train loss:\", loss_train[best_iteration], \"val loss:\", loss_val[best_iteration])"],"metadata":{"id":"oDFPo-ue9aXV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step 4: 学習済みモデルを使った推論\n","\n","学習済みのモデルを使って、\n","\n","*   testデータセットに対する推論をして精度評価をする\n","*   任意の画像を入力として推論する\n","\n","**validation datasetは既にハイパーパラメータの設定（best iterationの選択）に使ったので評価に使ってはいけない**\n","\n"],"metadata":{"id":"EJ-Ew9i7n4zE"}},{"cell_type":"markdown","source":["### testデータセットを使った精度評価"],"metadata":{"id":"t0N8MJVUn4uO"}},{"cell_type":"code","source":["# testデータセットからBoVWヒストグラム特徴を抽出\n","if os.path.exists('bovw_test.pkl'):\n","  print(\"Loading  from 'bovw_test.pkl'.\")\n","  with open('bovw_test.pkl','rb') as f:  \n","    probs_test, labels_test = pickle.load(f)\n","\n","else:\n","  print(\"Extracting BoVW histograms from test dataset\")\n","  probs_test, labels_test = calc_probs(test_dataset) # TEST dataset  \n","\n","  with open('bovw_test.pkl', 'wb') as f:\n","    pickle.dump([probs_test, labels_test], f)\n","\n","# testデータセット全画像に対し推論する\n","print(\"Predicting\")\n","start = time.perf_counter()\n","\n","x_test = np.vstack(probs_test)\n","y_test = labels_test\n","y_pred = gbm.predict(x_test,ntree_start=0,ntree_end=best_iteration)  # best iterationまでの木を使う\n","\n","print(\"Elapsed time:\",time.perf_counter()-start,\"[s]\")\n","\n","# 参考として、testとvalデータセット全画像に対しても推論する\n","print(\"Predicting for train and val datasets (for reference)\")\n","start = time.perf_counter()\n","\n","x_train = np.vstack(probs_train)\n","y_train = labels_train\n","y_pred_train = gbm.predict(x_train,ntree_start=0,ntree_end=best_iteration)  # best iterationまでの木を使う\n","\n","x_val = np.vstack(probs_val)\n","y_val = labels_val\n","y_pred_val = gbm.predict(x_val,ntree_start=0,ntree_end=best_iteration)  # best iterationまでの木を使う\n","\n","print(\"Elapsed time:\",time.perf_counter()-start,\"[s]\")\n","\n","# 精度（101クラス分類の正解率）を計算する\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n","\n","print(\"---Accuracy evaluation---\")\n","print(\"Test accuracy:\",accuracy_score(y_test,y_pred))\n","print(\"Train accuracy (for reference)':\",accuracy_score(y_train,y_pred_train))\n","print(\"Val accuracy (for reference):\",accuracy_score(y_val,y_pred_val))"],"metadata":{"id":"c6M2VbXfmYuM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 任意の画像に対する予測"],"metadata":{"id":"ptACaq3hoKrx"}},{"cell_type":"code","source":["path = \"wrightflyer.jpg\"\n","\n","img = cv2.imread(path,cv2.IMREAD_GRAYSCALE)\n","\n","# グリッド点上に特徴点を定義\n","keypoints = create_dense_keypoints(img,feat,step=feat_step,scale=feat_step)\n","# SIFT特徴量を計算し、一番近いクラスタを見つけ（FLANNによる近似最近傍探索）、画像中の全点でヒストグラムを作る\n","histogram = bowExtractor.compute(img, keypoints)[0]\n","\n","[[y_pred]] = gbm.predict([histogram])\n","\n","print(\"Predicted as\", categories[y_pred], \", category id:\",y_pred)\n","imshow(img)"],"metadata":{"id":"sCPXKle8oKZr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"26PbU2k9UT2F"},"execution_count":null,"outputs":[]}]}